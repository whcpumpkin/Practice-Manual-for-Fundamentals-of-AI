### **1. 张量（Tensor）**
- **定义**：PyTorch中的基本数据结构，是多维数组的扩展，支持GPU加速计算和自动微分。
- **关键点**：
  - 类似NumPy的`ndarray`，但可通过`.to(device)`迁移到GPU或者其它加速卡（昇腾、寒武纪MLU）。
  - 创建方式：`torch.tensor(data)`, `torch.zeros()`, `torch.rand()`等。或者从`List`和`np.arrary`转换，`torch.tensor(data_list)`或`torch.from_numpy(data_np)`。
  - 支持数学运算（如加减乘除、矩阵乘法`@`或`torch.matmul()`）。
  - 形状操作：`view()`（需连续内存）、`reshape()`（自动处理非连续内存）。
  - 类型转换：`float()`、`long()`、`int()`、`bool()`。
- **示例**：
  ```python
  x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, requires_grad=True,device="cpu") //创建一个形状为(2, 2)的张量，数据类型为float32，可求导。
  y = x.view(1, 4) //将张量形状变为(1, 4)
  z = x.int() //将张量数据类型变为int
  w = x.to("cuda") //将张量迁移到GPU
  x2 = torch.tensor([1, 2, 3, 4], dtype=torch.float32)
  x3 = x2.reshape(2, 2) //将张量形状变为(2, 2)
  x4 = x3.transpose(0, 1) //将张量维度交换
  x5 = x2.unsqueeze(0) //在第0维增加维度
  x6 = x2.squeeze(0) //在第0维减少维度
  x7 = torch.cat([x, x3], dim=0) //按维度0拼接张量
  ```

---

### **2. 自动微分（Autograd）**
- **定义**：PyTorch通过`autograd`模块自动计算梯度，用于反向传播。
- **关键点**：
  - `requires_grad=True`：标记需要计算梯度的张量。
  - `backward()`：从标量张量触发梯度计算，结果存储在`.grad`属性中。
  - 计算图：动态构建，每次前向传播生成新图。
  - 上下文管理器：`torch.no_grad()`禁用梯度跟踪（如推理时）。
  - 我们强烈推荐跑一下中的代码`0-基础概念.ipynb`
- **示例**：
  ```python
  x = torch.tensor([1., 2.], requires_grad=True)
  y = torch.tensor([3., 4.], requires_grad=True)
  z = x * y
  z.backward() //计算梯度
  print(x.grad) //输出梯度值
  print(y.grad) //输出梯度值
  ```

---

### **3. 神经网络模块（nn.Module）**
- **定义**：所有神经网络层的基类，用于构建自定义模型。
- **关键点**：
  - 继承`nn.Module`并实现`forward()`方法定义前向传播。
  - 层定义：使用`nn.Linear`, `nn.Conv2d`等预定义层。
  - 参数管理：通过`parameters()`访问可学习参数。
- **示例**：
  ```python
  class Net(nn.Module):
      def __init__(self):
          super().__init__()
          self.fc = nn.Linear(10, 2)
      def forward(self, x):
          return self.fc(x)
  ```

---

### **4. 损失函数（Loss Functions）**
- **定义**：衡量模型输出与目标的差异，用于优化参数。
- **常见损失函数**：
  - 分类任务：`nn.CrossEntropyLoss()`
  - 回归任务：`nn.MSELoss()`
- **使用方式**：
  ```python
  criterion = nn.CrossEntropyLoss()
  loss = criterion(output, target)
  ```

---

### **5. 优化器（Optimizer）**
- **定义**：更新模型参数以减少损失，如SGD、Adam。
- **关键点**：
  - 从`torch.optim`导入，如`optim.SGD(model.parameters(), lr=0.01)`。
  - `step()`：执行参数更新。
  - `zero_grad()`：清空梯度（防止累积）。
- **示例**：
  ```python
  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
  ```

---

### **6. 数据加载（Dataset & DataLoader）**
- **定义**：高效加载和预处理数据。
- **关键类**：
  - `Dataset`：抽象数据集，需实现`__len__`和`__getitem__`。
  - `DataLoader`：生成批量数据，支持多进程加载。
- **示例**：
  ```python
  dataset = CustomDataset()
  dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
  ```

---

### **7. GPU加速（CUDA）**
- **定义**：利用GPU加速计算。
- **操作**：
  - 设备选择：`device = torch.device("cuda" if torch.cuda.is_available() else "cpu")`。
  - 迁移数据/模型：`tensor.to(device)`或`model.to(device)`。
- **注意**：确保输入数据和模型在同一设备上。

---

### **8. 训练循环（Training Loop）**
- **流程**：
  1. 前向传播：`output = model(input)`
  2. 计算损失：`loss = criterion(output, target)`
  3. 反向传播：`loss.backward()`
  4. 参数更新：`optimizer.step()`
  5. 梯度清零：`optimizer.zero_grad()`
- **模式切换**：
  - `model.train()`：训练模式（启用Dropout等）。
  - `model.eval()`：评估模式（禁用Dropout）。

---

### **9. 模型保存与加载**
- **方法**：
  - 保存：`torch.save(model.state_dict(), "model.pth")`
  - 加载：`model.load_state_dict(torch.load("model.pth"))`
- **注意**：保存时需包含模型结构和参数。

---

### **10. 动态计算图（Dynamic Computation Graph）**
- **特点**：每次前向传播实时构建计算图，灵活支持条件分支、循环等控制流。
- **优势**：调试方便，适合动态网络结构（如RNN）。

---

### **总结步骤**
1. **定义模型**（继承`nn.Module`）。
2. **准备数据**（使用`Dataset`和`DataLoader`）。
3. **选择损失函数和优化器**。
4. **编写训练循环**（前向传播、计算损失、反向传播、参数更新）。
5. **评估模型**，保存/加载模型参数。

通过掌握这些概念，你将能够构建并训练基本的神经网络模型。建议结合官方文档和实战项目（如MNIST分类）巩固理解。