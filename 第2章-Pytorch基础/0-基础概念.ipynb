{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. 张量（Tensor）**\n",
    "- **定义**：PyTorch中的基本数据结构，是多维数组的扩展，支持GPU加速计算和自动微分。\n",
    "- **关键点**：\n",
    "  - 类似NumPy的`ndarray`，但可通过`.to(device)`迁移到GPU或者其它加速卡（昇腾、寒武纪MLU）。\n",
    "  - 创建方式：`torch.tensor(data)`, `torch.zeros()`, `torch.rand()`等。或者从`List`和`np.arrary`转换，`torch.tensor(data_list)`或`torch.from_numpy(data_np)`。\n",
    "  - 支持数学运算（如加减乘除、矩阵乘法`@`或`torch.matmul()`）。\n",
    "  - 形状操作：`view()`（需连续内存）、`reshape()`（自动处理非连续内存）。\n",
    "  - 类型转换：`float()`、`long()`、`int()`、`bool()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x 形状: torch.Size([2, 2])\n",
      "x 数据类型: torch.float32\n",
      "y 形状: torch.Size([1, 4])\n",
      "z 数据类型: torch.int32\n",
      "w 设备: cuda:0\n",
      "x2 形状: torch.Size([4])\n",
      "x3 形状: torch.Size([2, 2])\n",
      "x4 形状: torch.Size([2, 2])\n",
      "x5 形状: torch.Size([1, 4])\n",
      "x6 形状: torch.Size([4])\n",
      "x7 形状: torch.Size([4, 2])\n",
      "x7 元素: tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [1., 2.],\n",
      "        [3., 4.]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个形状为(2, 2)的张量，数据类型为float32，可求导。\n",
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, requires_grad=True,device=\"cpu\")\n",
    "print(\"x 形状:\", x.shape)\n",
    "print(\"x 数据类型:\", x.dtype)\n",
    "y = x.view(1, 4)\n",
    "print(\"y 形状:\", y.shape)\n",
    "z = x.int()\n",
    "print(\"z 数据类型:\", z.dtype)\n",
    "w = x.to(\"cuda\")\n",
    "print(\"w 设备:\", w.device)\n",
    "x2 = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "x3 = x2.reshape(2, 2)\n",
    "print(\"x2 形状:\", x2.shape)\n",
    "print(\"x3 形状:\", x3.shape)\n",
    "x4 = x3.transpose(0, 1)\n",
    "print(\"x4 形状:\", x4.shape)\n",
    "x5 = x2.unsqueeze(0)\n",
    "print(\"x5 形状:\", x5.shape)\n",
    "x6 = x2.squeeze(0)\n",
    "print(\"x6 形状:\", x6.shape)\n",
    "x7 = torch.cat([x, x3], dim=0)\n",
    "print(\"x7 形状:\", x7.shape)\n",
    "print(\"x7 元素:\", x7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. 自动微分（Autograd）**\n",
    "- **定义**：PyTorch通过`autograd`模块自动计算梯度，用于反向传播。\n",
    "- **关键点**：\n",
    "  - `requires_grad=True`：标记需要计算梯度的张量。\n",
    "  - `backward()`：从标量张量触发梯度计算，结果存储在`.grad`属性中。\n",
    "  - 计算图：动态构建，每次前向传播生成新图。\n",
    "  - 上下文管理器：`torch.no_grad()`禁用梯度跟踪（如推理时）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([3., 8.], grad_fn=<MulBackward0>)\n",
      "x.grad: tensor([3., 4.])\n",
      "y.grad: tensor([1., 2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.], requires_grad=True)\n",
    "y = torch.tensor([3., 4.], requires_grad=True)\n",
    "z = x * y\n",
    "print(z.shape)\n",
    "print(z)\n",
    "z.sum().backward() #计算梯度\n",
    "print(\"x.grad:\", x.grad) #输出梯度值\n",
    "print(\"y.grad:\", y.grad) #输出梯度值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下方的代码当中，我们构建了一个简单的神经网络。\n",
    "并使用 **均方误差（MSELoss）** 计算损失。我们可以用 **LaTeX** 语言来表示前向传播和反向传播（梯度计算）的公式。  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. 前向传播**\n",
    "设：\n",
    "- **输入** $ x \\in \\mathbb{R}^{bs\\times 3} $，其中 $ bs $ 是批量大小\n",
    "- **第一层权重** $ W_1 \\in \\mathbb{R}^{5 \\times 3} $, **第一层偏置** $ b_1 \\in \\mathbb{R}^5 $\n",
    "- **第二层权重** $ W_2 \\in \\mathbb{R}^{1 \\times 5} $, **第二层偏置** $ b_2 \\in \\mathbb{R}^1 $\n",
    "- **ReLU 激活函数** $ \\sigma(x) = \\max(0, x) $\n",
    "\n",
    "则：\n",
    "\n",
    "$h_1 = W_1 x + b_1 \\\\$\n",
    "$a_1 = \\sigma(h_1) = \\max(0, h_1)\\\\$\n",
    "$h_2 = W_2 a_1 + b_2\\\\$\n",
    "$y_{\\text{pred}} = h_2\\\\$\n",
    "\n",
    "### **2. 损失函数**\n",
    "损失使用 **均方误差（MSE）**：\n",
    "$\\mathcal{L} = (y_{\\text{pred}} - y_{\\text{true}})^2\\\\$\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 反向传播**\n",
    "#### **计算梯度**\n",
    "1. **损失对最终输出** $ h_2 $ 的导数：\n",
    "   $\\\\\\frac{\\partial \\mathcal{L}}{\\partial h_2} = 2 (h_2 - y_{\\text{true}})\\\\$\n",
    "\n",
    "2. **对第二层权重 $ W_2 $ 和偏置 $ b_2 $ 计算梯度**：\n",
    "   $\\\\\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial W_2} = \\frac{\\partial \\mathcal{L}}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial W_2}\n",
    "   = 2 (h_2 - y_{\\text{true}}) \\cdot a_1^T\n",
    "   \\\\$\n",
    "   $\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial b_2} = \\frac{\\partial \\mathcal{L}}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial b_2}\n",
    "   = 2 (h_2 - y_{\\text{true}})\n",
    "   $\n",
    "\n",
    "3. **对第一层激活 $ a_1 $ 计算梯度**：\n",
    "   $\n",
    "\\\\\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial a_1} = \\frac{\\partial \\mathcal{L}}{\\partial h_2} \\cdot W_2\n",
    "   = 2 (h_2 - y_{\\text{true}}) W_2\n",
    "   $\n",
    "\n",
    "4. **对第一层的输入 $ h_1 $ 计算梯度（考虑 ReLU）**：\n",
    "   $\n",
    "   \\\\\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial h_1} = \\frac{\\partial \\mathcal{L}}{\\partial a_1} \\odot \\sigma'(h_1)\\\\$\n",
    "   其中\n",
    "   $\n",
    "   \\sigma'(h_1) =\n",
    "   \\begin{cases}\n",
    "   1, & h_1 > 0 \\\\\n",
    "   0, & h_1 \\leq 0\n",
    "   \\end{cases}\n",
    "   \\\\$\n",
    "   因此：\n",
    "   $\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial h_1} = \\left(2 (h_2 - y_{\\text{true}}) W_2\\right) \\odot \\mathbb{1}(h_1 > 0)\n",
    "   $\n",
    "\n",
    "5. **对第一层权重 $ W_1 $ 和偏置 $ b_1 $ 计算梯度**：\n",
    "   $\\\\\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial W_1} = \\frac{\\partial \\mathcal{L}}{\\partial h_1} \\cdot x\n",
    "   \\\\$\n",
    "   $\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial b_1} = \\frac{\\partial \\mathcal{L}}{\\partial h_1}\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### **最终梯度更新**\n",
    "在反向传播过程中，PyTorch 会自动计算这些梯度，并用于 **梯度下降优化**：\n",
    "$\n",
    "W_i = W_i - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W_i}\n",
    "$\n",
    "$\n",
    "b_i = b_i - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b_i}\n",
    "$\n",
    "其中 $ \\eta $ 是学习率。\n",
    "\n",
    "---\n",
    "\n",
    "这样，你的神经网络就完成了一次前向传播、损失计算和反向传播的完整数学过程。🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "h1: tensor([[-0.7639,  0.5218,  1.3314, -0.7480,  0.2545]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "a1: tensor([[0.0000, 0.5218, 1.3314, 0.0000, 0.2545]], grad_fn=<ReluBackward0>)\n",
      "h2: tensor([[-0.5780]], grad_fn=<AddmmBackward0>)\n",
      "y_true: tensor([[1.]])\n",
      "y_pred: tensor([[-0.5780]], grad_fn=<AddmmBackward0>)\n",
      "loss: tensor(2.4900, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "\n",
      "------Gradient of Layer_2------\n",
      "Auto Layer_2.weight.grad: tensor([[ 0.0000, -1.6468, -4.2019,  0.0000, -0.8033]])\n",
      "Manual Layer_2.weight.grad: tensor([[-0.0000],\n",
      "        [-1.6468],\n",
      "        [-4.2019],\n",
      "        [-0.0000],\n",
      "        [-0.8033]], grad_fn=<MulBackward0>)\n",
      "Auto Layer_2.bias.grad: tensor([-3.1559])\n",
      "Manual Layer_2.bias.grad: tensor([[-3.1559]], grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "------Gradient of Layer_1------\n",
      "First Row of Auto Layer_1.weight.grad: tensor([-0.1434, -1.4336,  0.5011])\n",
      "First Row of Manual Layer_1.weight.grad: tensor([-0.1434, -1.4336,  0.5011], grad_fn=<SliceBackward0>)\n",
      "Auto Layer_1.bias.grad: tensor([ 0.0000, -0.8949,  0.7276,  0.0000,  0.7865])\n",
      "Manual Layer_1.bias.grad: tensor([ 0.0000, -0.8949,  0.7276,  0.0000,  0.7865], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "Layer_1 = torch.nn.Linear(3, 5)\n",
    "Relu = torch.nn.ReLU()\n",
    "Layer_2 = torch.nn.Linear(5, 1)\n",
    "Loss_func = torch.nn.MSELoss()\n",
    "\n",
    "# 定义输入\n",
    "x = torch.randn((1,3))\n",
    "print(x.shape)\n",
    "h1 = Layer_1(x)\n",
    "print(\"h1:\", h1)\n",
    "a1 = Relu(h1)\n",
    "print(\"a1:\", a1)\n",
    "h2 = Layer_2(a1)\n",
    "print(\"h2:\", h2)\n",
    "\n",
    "# 定义输出\n",
    "y_true = torch.tensor([[1]], dtype=torch.float32)\n",
    "print(\"y_true:\", y_true)\n",
    "y_pred = h2\n",
    "print(\"y_pred:\", y_pred)\n",
    "\n",
    "# 计算损失\n",
    "loss = Loss_func(y_pred, y_true)\n",
    "print(\"loss:\", loss)\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "print()\n",
    "print()\n",
    "print(\"------Gradient of Layer_2------\")\n",
    "\n",
    "\n",
    "print(\"Auto Layer_2.weight.grad:\", Layer_2.weight.grad)\n",
    "print(\"Manual Layer_2.weight.grad:\", 2 * (y_pred - y_true) * a1.t())\n",
    "print(\"Auto Layer_2.bias.grad:\", Layer_2.bias.grad)\n",
    "print(\"Manual Layer_2.bias.grad:\", 2 * (y_pred - y_true))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"------Gradient of Layer_1------\")\n",
    "\n",
    "h1_grad = 2 * (y_pred - y_true) * Layer_2.weight *h1.gt(0)\n",
    "print(\"First Row of Auto Layer_1.weight.grad:\", Layer_1.weight.grad[1,:])\n",
    "print(\"First Row of Manual Layer_1.weight.grad:\", (h1_grad.t()*x)[1,:])\n",
    "print(\"Auto Layer_1.bias.grad:\", Layer_1.bias.grad)\n",
    "print(\"Manual Layer_1.bias.grad:\", h1_grad.sum(0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aibasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
