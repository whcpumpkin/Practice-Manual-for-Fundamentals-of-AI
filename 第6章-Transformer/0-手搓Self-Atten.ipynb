{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from torchviz import make_dot\n",
    "# 设置随机种子以保证结果可重现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_simplified(x):\n",
    "    \"\"\"\n",
    "    简化版的Self-Attention实现\n",
    "    参数:\n",
    "        x: 输入张量，形状为 [batch_size, seq_len, d_model]\n",
    "           batch_size: 批量大小\n",
    "           seq_len: 序列长度\n",
    "           d_model: 特征维度\n",
    "    返回:\n",
    "        attention_output: Self-Attention的输出\n",
    "    \"\"\"\n",
    "    # 步骤 1: 计算查询(Query)、键(Key)、值(Value)矩阵\n",
    "    # 在实际Transformer中，这些矩阵是通过学习的线性变换得到的\n",
    "    # 但这里为了简单，我们直接使用输入作为Q、K、V\n",
    "    query = x\n",
    "    key = x\n",
    "    value = x\n",
    "    \n",
    "    # 步骤 2: 计算注意力分数 (Attention Scores)\n",
    "    # 公式: scores = Q × K^T\n",
    "    # 其中 Q 和 K^T 的维度分别为 [batch_size, seq_len, d_model] 和 [batch_size, d_model, seq_len]\n",
    "    scores = torch.bmm(query, key.transpose(1, 2))\n",
    "    \n",
    "    print(\"scores.shape:\", scores.shape)\n",
    "    print(\"scores:\", scores)\n",
    "    \n",
    "    # 步骤 3: 对分数进行缩放 (Scaling)\n",
    "    # 公式: scaled_scores = scores / sqrt(d_model)\n",
    "    d_model = query.size(-1)\n",
    "    scaled_scores = scores / torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "    \n",
    "    # 步骤 4: 通过softmax函数获取注意力权重\n",
    "    # 在序列维度(dim=2)上应用softmax，把注意力分数变成概率分布\n",
    "    attention_weights = torch.softmax(scaled_scores, dim=-1)\n",
    "    \n",
    "    print(\"attention_weights.shape:\", attention_weights.shape)\n",
    "    print(\"attention_weights:\", attention_weights)\n",
    "    \n",
    "    # 步骤 5: 将注意力权重与Value矩阵相乘，得到最终的输出\n",
    "    # 公式: output = attention_weights × V\n",
    "    attention_output = torch.bmm(attention_weights, value)\n",
    "    \n",
    "    return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入序列 x 的形状: torch.Size([1, 3, 4])\n",
      "输入序列的值:\n",
      " tensor([[[ 1.1103, -1.6898, -0.9890,  0.9580],\n",
      "         [ 1.3221,  0.8172, -0.7658, -0.7506],\n",
      "         [ 1.3525,  0.6863, -0.3278,  0.7950]]])\n",
      "scores.shape: torch.Size([1, 3, 3])\n",
      "scores: tensor([[[5.9839, 0.1254, 1.4277],\n",
      "         [0.1254, 3.5658, 2.0034],\n",
      "         [1.4277, 2.0034, 3.0398]]])\n",
      "attention_weights.shape: torch.Size([1, 3, 3])\n",
      "attention_weights: tensor([[[0.8651, 0.0462, 0.0887],\n",
      "         [0.1094, 0.6109, 0.2797],\n",
      "         [0.2187, 0.2916, 0.4897]]])\n",
      "\n",
      "Self-Attention输出的形状: torch.Size([1, 3, 4])\n",
      "Self-Attention输出的值:\n",
      " tensor([[[ 1.1416, -1.3633, -0.9200,  0.8645],\n",
      "         [ 1.3075,  0.5064, -0.6677, -0.1314],\n",
      "         [ 1.2907,  0.2049, -0.6001,  0.3799]]])\n"
     ]
    }
   ],
   "source": [
    "# 2. 创建一个简单的例子来演示self-attention\n",
    "# 定义一个包含3个token的序列，每个token用4维向量表示\n",
    "batch_size = 1  # 批量大小为1（处理单个序列）\n",
    "seq_len = 3     # 序列长度为3（比如3个单词）\n",
    "d_model = 4     # 每个token的特征维度为4\n",
    "\n",
    "# 创建一个随机输入张量\n",
    "x = torch.randn(batch_size,seq_len, d_model)\n",
    "\n",
    "print(\"输入序列 x 的形状:\", x.shape)\n",
    "print(\"输入序列的值:\\n\", x)\n",
    "\n",
    "# 3. 应用self-attention函数\n",
    "output, attention_weights = self_attention_simplified(x)\n",
    "\n",
    "print(\"\\nSelf-Attention输出的形状:\", output.shape)\n",
    "print(\"Self-Attention输出的值:\\n\", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aibasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
