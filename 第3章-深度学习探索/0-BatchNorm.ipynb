{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批归一化（Batch Normalization）技术详解\n",
    "\n",
    "## 一、概述\n",
    "批归一化（Batch Normalization，简称BatchNorm）是深度学习领域的重要技术之一，由Sergey Ioffe和Christian Szegedy于2015年提出。该技术通过规范化神经网络中间层的输入分布，显著提升了深度神经网络的训练速度和模型稳定性。\n",
    "\n",
    "## 二、核心原理\n",
    "### 1. 基本公式\n",
    "\n",
    "#### 输入数据格式\n",
    "- 矩阵形状：`(B, D)`\n",
    "- B = Batch Size（样本数量）\n",
    "- D = Feature Dimension（特征维度）\n",
    "\n",
    "#### 计算步骤分解\n",
    "\n",
    "### 1. 计算特征维度上的统计量\n",
    "对每个特征维度**独立计算**批统计量：\n",
    "\n",
    "$\\mu_d = \\frac{1}{B}\\sum_{b=1}^B x_{b,d} \\quad \\forall d \\in [1,D]$\n",
    "\n",
    "$\\sigma_d^2 = \\frac{1}{B}\\sum_{b=1}^B (x_{b,d}-\\mu_d)^2 \\quad \\forall d \\in [1,D]$\n",
    "\n",
    "### 2. 归一化\n",
    "\n",
    "$y_b = \\frac{x_b-\\mu}{\\sqrt{\\sigma^2+\\epsilon}} \\quad \\forall b \\in [1,B]$\n",
    "\n",
    "其中，$\\epsilon$是一个很小的常数，防止分母为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是初始化的特征向量：\n",
      "[[0.13443642 0.38237412 0.43459833 0.5555555  0.46544354]\n",
      " [0.73888243 0.76263731 0.2219813  0.65330424 0.2280716 ]\n",
      " [0.16306301 0.62442583 0.54017516 0.64500589 0.29199674]\n",
      " [0.9398204  0.87478007 0.10693309 0.75469117 0.82808138]]\n",
      "以下是用torch.nn.BatchNorm1d归一化后的特征向量：\n",
      "tensor([[-1.0196, -1.5167,  0.6368, -1.3680,  0.0517],\n",
      "        [ 0.6942,  0.5529, -0.6090,  0.0165, -0.9665],\n",
      "        [-0.9385, -0.1993,  1.2554, -0.1010, -0.6923],\n",
      "        [ 1.2639,  1.1632, -1.2831,  1.4526,  1.6072]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "以下是用for 循环、mean和var实现归一化后的特征向量：\n",
      "[[-1.01966566 -1.51693113  0.63686758 -1.36941136  0.05167255]\n",
      " [ 0.69420696  0.55294339 -0.60911539  0.01651852 -0.96662445]\n",
      " [-0.93849655 -0.19937877  1.25557123 -0.10113964 -0.69239332]\n",
      " [ 1.26395525  1.16336652 -1.28332342  1.45403248  1.60734521]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "feature_vector = np.random.rand(4, 5) # 初始化一个4x5的随机特征向量，4是batch_size，5是特征维度\n",
    "print(\"以下是初始化的特征向量：\")\n",
    "print(feature_vector)\n",
    "\n",
    "# 用torch.nn.BatchNorm1d对特征进行归一化\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "batch_norm = nn.BatchNorm1d(5) # 实例化BatchNorm1d，输入特征维度为5\n",
    "feature_vector_tensor = torch.tensor(feature_vector,dtype=torch.float) # 将numpy数组转换为tensor\n",
    "torch_normalized_feature_vector = batch_norm(feature_vector_tensor) # 归一化特征\n",
    "print(\"以下是用torch.nn.BatchNorm1d归一化后的特征向量：\")\n",
    "print(torch_normalized_feature_vector) # 输出归一化后的特征向量\n",
    "\n",
    "# 用for 循环、mean和var实现归一化\n",
    "normalized_feature_vector = []\n",
    "for channel in range(5):\n",
    "    sum=0\n",
    "    for batch_idx in range(4):\n",
    "        sum += feature_vector[batch_idx][channel]\n",
    "    mean = sum/4\n",
    "    var = 0\n",
    "    for batch_idx in range(4):\n",
    "        var += (feature_vector[batch_idx][channel]-mean)**2\n",
    "    var = var/4\n",
    "    std = np.sqrt(var)\n",
    "    normalized_channel = (feature_vector[:,channel]-mean)/std\n",
    "    normalized_feature_vector.append(normalized_channel)\n",
    "for_normalized_feature_vector = np.array(normalized_feature_vector).T\n",
    "print(\"以下是用for 循环、mean和var实现归一化后的特征向量：\")\n",
    "print(for_normalized_feature_vector) # 输出归一化后的特征向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、理论推导之后的代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[BN模型] Lipschitz常数中位数: 0.00, 最大值: 7799.11\n",
      "[无BN模型] Lipschitz常数中位数: 0.00, 最大值: 2415774.36\n",
      "==================================================\n",
      "\n",
      "特征统计量对比：\n",
      "原始数据 - 均值: [1.5890496e+00 7.5324074e+01 3.4941802e+03]\n",
      "原始数据 - 标准差: [2.3224361e-01 2.5581488e+01 8.5197577e+02]\n",
      "\n",
      "BN层后激活值 - 均值: [ 0.00576357  0.00562656 -0.00516746]\n",
      "BN层后激活值 - 标准差: [1.0035264 1.0027452 1.0029914]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 参数设置\n",
    "input_dim = 3       # 输入特征维度（身高、体重、肺活量）\n",
    "hidden_dim = 64     # 隐藏层维度\n",
    "num_samples = 100   # 减少采样次数以加快计算\n",
    "batch_size = 128    # 添加批处理支持\n",
    "epsilon = 1e-5      # 扰动幅度\n",
    "dataset_size = 1000 # 训练集大小\n",
    "\n",
    "# 生成符合要求的训练数据\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 生成特征数据 (1000×3)\n",
    "height = 1.2 + (2.0 - 1.2) * torch.rand(dataset_size, 1)        # 1.2-2.0米\n",
    "weight = 30 + (120 - 30) * torch.rand(dataset_size, 1)          # 30-120千克\n",
    "vital_capacity = 2000 + (5000 - 2000) * torch.rand(dataset_size, 1)  # 2000-5000毫升\n",
    "x = torch.cat([height, weight, vital_capacity], dim=1)\n",
    "\n",
    "# 生成目标分数（基于非线性关系+噪声）\n",
    "y = (height * 35 + torch.log(weight) * 15 + vital_capacity * 0.008)  # 基础分\n",
    "y += torch.randn_like(y) * 10  # 添加噪声\n",
    "y = torch.clamp(y, 0, 100)     # 裁剪到0-100分\n",
    "\n",
    "# 定义带BN的模型\n",
    "class BNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# 定义不带BN的模型\n",
    "class NoBNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# 初始化模型\n",
    "bn_model = BNModel()\n",
    "no_bn_model = NoBNModel()\n",
    "\n",
    "# 改进的BN预热（使用数据加载器）\n",
    "class TensorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 预热BN层（模拟真实训练）\n",
    "bn_model.train()\n",
    "for _ in range(50):  # 50个epoch的预热\n",
    "    for batch_x, _ in loader:\n",
    "        bn_model(batch_x)\n",
    "bn_model.eval()\n",
    "\n",
    "# 优化的L估计函数（支持批处理）\n",
    "def estimate_L(model, x, y):\n",
    "    model.zero_grad()\n",
    "    params = list(model.parameters())\n",
    "    originals = [p.data.clone() for p in params]\n",
    "    L = []\n",
    "    \n",
    "    # 使用相同随机种子保证扰动一致性\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # 生成参数扰动\n",
    "        deltas = [torch.randn_like(p) * epsilon for p in params]\n",
    "        \n",
    "        # 应用扰动并计算梯度\n",
    "        for p, delta in zip(params, deltas):\n",
    "            p.data += delta\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = ((y_pred - y)**2).mean()\n",
    "        loss.backward()\n",
    "        grads1 = [p.grad.clone() for p in params]\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 恢复参数并计算原始梯度\n",
    "        for p, orig in zip(params, originals):\n",
    "            p.data = orig\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = ((y_pred - y)**2).mean()\n",
    "        loss.backward()\n",
    "        grads2 = [p.grad.clone() for p in params]\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 计算范数\n",
    "        grad_diff = sum(torch.norm(g1 - g2, p=2).item()**2 for g1, g2 in zip(grads1, grads2))**0.5\n",
    "        delta_norm = sum(torch.norm(d, p=2).item()**2 for d in deltas)**0.5\n",
    "        \n",
    "        if delta_norm > 1e-7:\n",
    "            L.append(grad_diff / delta_norm)\n",
    "    \n",
    "    return np.median(L), np.max(L)\n",
    "\n",
    "# 计算结果\n",
    "bn_median, bn_max = estimate_L(bn_model, x, y)\n",
    "no_bn_median, no_bn_max = estimate_L(no_bn_model, x, y)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"[BN模型] Lipschitz常数中位数: {bn_median:.2f}, 最大值: {bn_max:.2f}\")\n",
    "print(f\"[无BN模型] Lipschitz常数中位数: {no_bn_median:.2f}, 最大值: {no_bn_max:.2f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 特征标准化情况对比\n",
    "print(\"\\n特征统计量对比：\")\n",
    "print(\"原始数据 - 均值:\", x.mean(dim=0).detach().numpy())\n",
    "print(\"原始数据 - 标准差:\", x.std(dim=0).detach().numpy())\n",
    "\n",
    "# 查看BN层后的激活值统计\n",
    "bn_output = bn_model.fc1(x)\n",
    "bn_output = bn_model.bn(bn_output)\n",
    "print(\"\\nBN层后激活值 - 均值:\", bn_output.mean(dim=0).detach().numpy()[:3])\n",
    "print(\"BN层后激活值 - 标准差:\", bn_output.std(dim=0).detach().numpy()[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aibasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
